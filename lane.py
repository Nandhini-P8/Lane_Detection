# -*- coding: utf-8 -*-
"""lane

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CdOvIFZBibNCzWRtnlNtKdq9n6vO8jIJ
"""

!pip install tensorflow opencv-python-headless matplotlib

import cv2
import numpy as np
from google.colab.patches import cv2_imshow

def grayscale(img):
    return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

video_path = '/content/test2.mp4'
cap = cv2.VideoCapture(video_path)

while cap.isOpened():
    ret, frame = cap.read()
    if ret:
        gray_frame = grayscale(frame)
        cv2_imshow(gray_frame)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
    else:
        break
cap.release()
cv2.destroyAllWindows()

video_path = '/content/test2.mp4'
cap = cv2.VideoCapture(video_path)

def gaussian_blur(img, kernel_size=5):
    return cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)

def canny_edge_detection(img, low_threshold=50, high_threshold=150):
    return cv2.Canny(img, low_threshold, high_threshold)

def region_of_interest(img, vertices):
    mask = np.zeros_like(img)
    cv2.fillPoly(mask, vertices, 255)
    masked_image = cv2.bitwise_and(img, mask)
    return masked_image

import cv2
import numpy as np
video_path = '/content/test2.mp4'
cap = cv2.VideoCapture(video_path)
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break
    img = cv2.resize(frame, (256, 256))
    img = img / 255.0
    img = np.expand_dims(img, axis=0)
    mask = model.predict(img)[0]
    mask = (mask * 255).astype(np.uint8)
    mask = cv2.resize(mask, (frame.shape[1], frame.shape[0]))
    result = cv2.addWeighted(frame, 1, cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR), 0.5, 0)
    cv2_imshow(result)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break
cap.release()
cv2.destroyAllWindows()

import cv2
import os
video_path = "/content/test2.mp4"
output_folder = "/content/frames"
if not os.path.exists(output_folder):
    os.makedirs(output_folder)
cap = cv2.VideoCapture(video_path)
frame_number = 0
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break
    frame_filename = os.path.join(output_folder, f"frame_{frame_number:04d}.png")
    cv2.imwrite(frame_filename, frame)
    frame_number += 1
cap.release()
cv2.destroyAllWindows()
print(f"Extracted {frame_number} frames to {output_folder}")

while cap.isOpened():
    ret, frame = cap.read()
    if ret:
        height, width = frame.shape[:2]
        vertices = np.array([[(0, height), (width / 2, int(height / 2)), (width, height)]], dtype=np.int32)
        roi_frame = region_of_interest(frame, vertices)
        cv2_imshow(roi_frame)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
    else:
        break
cap.release()
cv2.destroyAllWindows()

!pip install --upgrade tensorflow

import tensorflow as tf

def build_unet(input_shape=(256, 256, 3)):
  """Builds a U-Net model."""

  inputs = tf.keras.Input(shape=input_shape)
  conv1 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)
  conv1 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(conv1)
  pool1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1)

  conv2 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(pool1)
  conv2 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(conv2)
  pool2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2)

  conv3 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(pool2)
  conv3 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(conv3)
  pool3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv3)

  conv4 = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(pool3)
  conv4 = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(conv4)
  pool4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv4)

  conv5 = tf.keras.layers.Conv2D(1024, (3, 3), activation='relu', padding='same')(pool4)
  conv5 = tf.keras.layers.Conv2D(1024, (3, 3), activation='relu', padding='same')(conv5)
  up6 = tf.keras.layers.Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(conv5)
  merge6 = tf.keras.layers.concatenate([conv4, up6], axis=3)
  conv6 = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(merge6)
  conv6 = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(conv6)

  up7 = tf.keras.layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv6)
  merge7 = tf.keras.layers.concatenate([conv3, up7], axis=3)
  conv7 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(merge7)
  conv7 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(conv7)

  up8 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv7)
  merge8 = tf.keras.layers.concatenate([conv2, up8], axis=3)
  conv8 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(merge8)
  conv8 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(conv8)

  up9 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv8)
  merge9 = tf.keras.layers.concatenate([conv1, up9], axis=3)
  conv9 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(merge9)
  conv9 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(conv9)

  outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(conv9)

  model = tf.keras.Model(inputs=[inputs], outputs=[outputs])
  return model
model = build_unet()
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.summary()

import tensorflow as tf

def load_and_preprocess_image(image_path, mask_path):
  """Loads and preprocesses an image and its corresponding mask."""
  image = tf.io.read_file(image_path)
  image = tf.image.decode_png(image, channels=3)
  image = tf.image.resize(image, [256, 256])
  image = tf.cast(image, tf.float32) / 255.0

  mask = tf.io.read_file(mask_path)
  mask = tf.image.decode_png(mask, channels=1)
  mask = tf.image.resize(mask, [256, 256])
  mask = tf.cast(mask, tf.float32) / 255.0

  return image, mask

def create_dataset(image_paths, mask_paths, batch_size=32):
  """Creates a TensorFlow dataset for training."""
  dataset = tf.data.Dataset.from_tensor_slices((image_paths, mask_paths))
  dataset = dataset.map(load_and_preprocess_image)
  dataset = dataset.batch(batch_size)
  dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)
  return dataset

# Example usage:
image_paths = ['/path/to/your/image1.png', '/path/to/your/image2.png']
mask_paths = ['/path/to/your/mask1.png', '/path/to/your/mask2.png']

train_dataset = create_dataset(image_paths, mask_paths)

image_paths = [f'/content/frames/frame_{i:04d}.png' for i in range(frame_number)]
mask_paths = [f'/content/masks/mask_{i:04d}.png' for i in range(frame_number)]

fourcc = cv2.VideoWriter_fourcc(*'XVID')
out = cv2.VideoWriter('output_video.avi', fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))
while cap.isOpened():
    ret, frame = cap.read()
    if ret:
        processed_frame = process_frame(frame)
        cv2_imshow(processed_frame)
        out.write(cv2.cvtColor(processed_frame, cv2.COLOR_GRAY2BGR))
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
    else:
        break
cap.release()
out.release()
cv2.destroyAllWindows()

frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = cap.get(cv2.CAP_PROP_FPS) or 20.0
fourcc = cv2.VideoWriter_fourcc(*'XVID')
output_video_path = 'output_video.avi'
out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))

if os.path.exists(output_video_path):
    print("Output video successfully created:", output_video_path)
    files.download(output_video_path)
else:
    print("Error: Output video not found.")

import cv2
import numpy as np
from google.colab.patches import cv2_imshow
import os
import tensorflow as tf

# --- U-Net Model Definition (as you defined previously) ---
def build_unet(input_shape=(256, 256, 3)):
    # ... (your U-Net model code here) ...
    return model

# --- Video Processing Functions ---
def process_frame(frame, model):
    """Applies lane detection and returns the processed frame."""
    # 1. Resize the frame
    img = cv2.resize(frame, (256, 256))
    # 2. Normalize pixel values
    img = img / 255.0
    # 3. Add batch dimension
    img = np.expand_dims(img, axis=0)
    # 4. Make prediction using your model
    mask = model.predict(img)[0]
    # 5. Convert the mask to 8-bit grayscale
    mask = (mask * 255).astype(np.uint8)
    # 6. Resize the mask to the original frame size
    mask = cv2.resize(mask, (frame.shape[1], frame.shape[0]))
    # 7. Overlay the mask on the original frame
    result = cv2.addWeighted(frame, 1, cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR), 0.5, 0)
    return result

# --- Main Execution ---
def main():
    video_path = '/content/test2.mp4'
    output_video_path = 'output_video.avi'

    # 1. Load the U-Net model
    model = build_unet()
    # ... (load model weights if needed) ...

    # 2. Open the video file
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print("Error: Could not open video file.")
        return

    # 3. Get video properties for VideoWriter
    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = cap.get(cv2.CAP_PROP_FPS) or 20.0  # Default to 20 fps if not available
    fourcc = cv2.VideoWriter_fourcc(*'XVID')

    # 4. Create VideoWriter object
    out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))

    # 5. Process video frames
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        processed_frame = process_frame(frame, model)
        cv2_imshow(processed_frame)  # Display processed frame in Colab
        out.write(processed_frame)  # Write to output video

        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    # 6. Release resources
    cap.release()
    out.release()
    cv2.destroyAllWindows()

    print(f"Output video saved to: {output_video_path}")

# Entry point
if __name__ == "__main__":
    main()